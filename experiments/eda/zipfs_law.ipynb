{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\author{Austin Gill}\n",
    "\\title{Exploratory Data Analysis -- Zipf's Law}\n",
    "\\maketitle\n",
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to get comfortable with the Jupyter notebook workflow in the context of this project, as well as an opportunity to build some of the scaffolding around the haiku dataset.\n",
    "There are a number of intricacies that have changed since I last used Jupyter, such as `nbconvert` attempting to use Inkscape to convert SVG images to PDFs before exporting the notebook to PDF!\n",
    "\n",
    "The questions this notebook attempts to answer are\n",
    "\n",
    "* Does Zipf's law hold for haiku?\n",
    "* Does Zipf's law hold after removing stop words?\n",
    "* Does Zipf's law hold after stemming/lemmatization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automagically reimport haikulib if it changes.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "import spacy\n",
    "\n",
    "from haikulib import data, utils, nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_nlp = spacy.load(\"en\", disable=[\"parser\", \"ner\"])\n",
    "pd.set_option('display.latex.repr', True)\n",
    "pd.set_option('display.latex.longtable', True)\n",
    "plt.rcParams[\"figure.figsize\"] = (16 * 0.6, 9 * 0.6)\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipf's Law\n",
    "\n",
    "Zipf's law states that the frequencies of words from a natural language corpus are inversely proportional to their rank in a frequency table. That is, a plot of their rank vs frequency on a log-log scale will be roughly linear.\n",
    "\n",
    "For example, The first word in the table below is twice as frequent as the second word, and three times as frequent as the third.\n",
    "\n",
    "| rank | value  | occurrences |\n",
    "|------|--------|-------------|\n",
    "| 1    | word 1 | 21          |\n",
    "| 2    | word 2 | 10          |\n",
    "| 3    | word 3 | 7           |\n",
    "\n",
    "A plot of this frequency table on a log-log scale is shown below. Notice that the plot is roughly linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = np.array([1, 2, 3])\n",
    "frequencies = np.array([21, 10, 7])\n",
    "\n",
    "plt.plot(np.log(ranks), np.log(frequencies))\n",
    "plt.plot(np.log(ranks), np.log(frequencies), \".\")\n",
    "\n",
    "plt.title(\"Example of Zipf's Law\")\n",
    "plt.xlabel(\"$\\log(rank)$\")\n",
    "plt.ylabel(\"$\\log(freq)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipf's Law for our Dataset\n",
    "\n",
    "One of the ways to represent a natural language corpus is with a bag-of-words representation, where all of the individual words of the corpus have been tossed in a container *without any surrounding context*.\n",
    "This is a natural representation for the present work, as examining word frequencies does not require context.\n",
    "\n",
    "For future work it will become necessary to represent words from the corpus as mathematical vectors.\n",
    "This will allow us to use a more mathematical treatment of the problem, and allows us to (ab)use useful properties of vector spaces.\n",
    "For example, there are pre-built models like [word2vec](https://jalammar.github.io/illustrated-word2vec/) or [GloVe](https://nlp.stanford.edu/projects/glove/) that would encode words like \"man\" and \"woman\" as vectors that are relatively close together, as opposed to, say, the vectors for \"man\" and \"dog\".\n",
    "Further, these vector representations have the property that the vector between \"man\" and \"woman\" is close to the vector between \"king\" and \"queen\".\n",
    "\n",
    "However, for the present work, a bag-of-words suffices.\n",
    "So we get a bag-of-words representation of the dataset, and construct the frequency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_table(bag, thing=\"word\"):\n",
    "    \"\"\"Get a frequency table representation of the given bag-of-<thing> representation.\"\"\"\n",
    "    assert isinstance(bag, Counter)\n",
    "    things, frequencies = zip(*sorted(bag.items(), key=operator.itemgetter(1), reverse=True))\n",
    "    things = np.array(things)\n",
    "    frequencies = np.array(frequencies)\n",
    "    ranks = np.arange(1, len(things) + 1)\n",
    "\n",
    "    freq_table = pd.DataFrame({\"rank\": ranks, thing: things, \"frequency\": frequencies})\n",
    "    return freq_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency table is just another view of the bag-of-words.\n",
    "It contains no new information, but allows us to more easily examine the mathematical relationships of word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = data.get_bag_of(kind=\"words\")\n",
    "freq_table = get_freq_table(bag)\n",
    "freq_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the ranks of each word vs their frequency on a log-log scale reveals that Zipf's law does seem to hold for most of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(freq_table[\"rank\"]), np.log(freq_table[\"frequency\"]), '.', markersize=3)\n",
    "\n",
    "plt.title(\"Haiku Word Frequency\")\n",
    "plt.xlabel(\"$\\log(rank)$\")\n",
    "plt.ylabel(\"$\\log(freq)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So then we find the words and their corresponding frequencies at the interesting points of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(df, column, values):\n",
    "    \"\"\"Gets the indices of values from the given column of the given dataframe.\"\"\"\n",
    "    indices = []\n",
    "    for value in values:\n",
    "        indices += df[column][df[column] == value].index.tolist()\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = get_indices(freq_table, \"word\", [\"the\", \"a\", \"of\", \"to\", \"i\", \"her\", \"his\"])\n",
    "interesting = freq_table.loc[indices]\n",
    "interesting.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately this, and much of the subsequent work involves a fair amount of manual tweaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    np.log(freq_table[\"rank\"]), np.log(freq_table[\"frequency\"]), \".\", markersize=3\n",
    ")\n",
    "\n",
    "# This should be a crime.\n",
    "x_adjust = np.array([0.1, -0.6, 0.15, -0.6, 0.2, -0.6, 0.0])\n",
    "y_adjust = np.array([1.0, -1.2, 1.0, -1.3, 1.0, -1.3, 1.0])\n",
    "\n",
    "for word, freq, rank, xa, ya in zip(\n",
    "    interesting[\"word\"],\n",
    "    interesting[\"frequency\"],\n",
    "    interesting[\"rank\"],\n",
    "    x_adjust,\n",
    "    y_adjust,\n",
    "):\n",
    "    plt.annotate(\n",
    "        word,\n",
    "        xy=(np.log(rank), np.log(freq) + ya / 20),\n",
    "        xytext=(np.log(rank) + xa, np.log(freq) + ya),\n",
    "        size=9,\n",
    "        arrowprops={\"arrowstyle\": \"-\", \"color\": \"k\"},\n",
    "    )\n",
    "\n",
    "plt.title(\"Haiku Word Frequency\")\n",
    "plt.xlabel(\"$\\log(rank)$\")\n",
    "plt.ylabel(\"$\\log(freq)$\")\n",
    "plt.ylim((-0.5, 11.9))\n",
    "# plt.savefig('zipfs-uncleaned.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipf's Law After Removing Stop Words\n",
    "\n",
    "Unfortunately, the preceding diagram isn't very interesting.\n",
    "That's because the most common words in natural language are often filler words with little individual meaning.\n",
    "In the context of natural language processing, these are called **stop words**.\n",
    "\n",
    "So we remove the stopwords from the bag of words and repeat our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stopword in nlp.STOPWORDS:\n",
    "    if stopword in bag:\n",
    "        del bag[stopword]\n",
    "\n",
    "freq_table = get_freq_table(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    np.log(freq_table[\"rank\"]), np.log(freq_table[\"frequency\"]), \".\", markersize=3\n",
    ")\n",
    "\n",
    "plt.title(\"Haiku Word Frequency\")\n",
    "plt.xlabel(\"$\\log(rank)$\")\n",
    "plt.ylabel(\"$\\log(freq)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot retains the same characteristics, albeit with a slightly less linear shape.\n",
    "\n",
    "After removing the stop words, the most frequent words start to show characteristics unique to haiku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = get_indices(freq_table, \"word\", [\"moon\", \"rain\", \"day\", \"night\", \"snow\", \"winter\", \"summer\", \"spring\", \"autumn\"])\n",
    "\n",
    "interesting = freq_table.loc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    np.log(freq_table[\"rank\"]), np.log(freq_table[\"frequency\"]), \".\", markersize=3\n",
    ")\n",
    "\n",
    "# This should also be a crime.\n",
    "x_adjust = np.array([-0.35, -0.9, -0.23, -0.9, -0.1, -0.7, 0.3, -0.7, 0.4])\n",
    "y_adjust = np.array([1.0, -1.0, 1.1, -1.1, 1.1, -1.4, 1.0, -1.45, 1.0])\n",
    "\n",
    "for word, freq, rank, xa, ya in zip(\n",
    "    interesting[\"word\"],\n",
    "    interesting[\"frequency\"],\n",
    "    interesting[\"rank\"],\n",
    "    x_adjust,\n",
    "    y_adjust,\n",
    "):\n",
    "    plt.annotate(\n",
    "        word,\n",
    "        xy=(np.log(rank), np.log(freq) + ya / 20),\n",
    "        xytext=(np.log(rank) + xa, np.log(freq) + ya),\n",
    "        size=8,\n",
    "        arrowprops={\"arrowstyle\": \"-\", \"color\": \"k\"},\n",
    "    )\n",
    "\n",
    "plt.title(\"Haiku Word Frequency\")\n",
    "plt.xlabel(\"$\\log(rank)$\")\n",
    "plt.ylabel(\"$\\log(freq)$\")\n",
    "plt.xlim((-0.5, 10.5))\n",
    "plt.ylim((-0.5, 9))\n",
    "# plt.savefig(\"zipfs-cleaned.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Zipf's law, this diagram isn't very revealing.\n",
    "But as exploratory data analysis undertaken to understand the haiku dataset, it is quite illuminating.\n",
    "\n",
    "We can immediately tell that weather and seasons are major themes in haiku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequencies After Stemming/Lemmatization\n",
    "\n",
    "There are two computational approaches for getting the root form of a word - stemming and lemmatization.\n",
    "\n",
    "Stemming involves a sequence of rules used to strip off suffixes of the word to reduce it to its stem - which notably might not be a word.\n",
    "For example, \"leaves\" and \"leaving\" might both be stemmed to form \"leav\".\n",
    "Further, because stemming operates by removing parts of the word, it would fail to stem \"better\" and \"good\" the same.\n",
    "\n",
    "Notably, stemming is unaware of the vocabulary.\n",
    "It is a purely algorithmic process of applying grammatical rules to remove prefixes and suffixes.\n",
    "\n",
    "Lemmatization on the other hand, is aware of vocabulary.\n",
    "It is a more sophisticated technique that returns the word to its base dictionary form via morphological analysis.\n",
    "Lemmatization is much more costly than stemming, and is often performed using a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = data.get_bag_of(kind='words')\n",
    "\n",
    "for stopword in nlp.STOPWORDS:\n",
    "    if stopword in bag:\n",
    "        del bag[stopword]\n",
    "\n",
    "feq_table = get_freq_table(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "There are many approaches to stemming words, but the most common approaches are the Porter, Lancaster, and Snowball stemmers.\n",
    "\n",
    "So in order to get a feel for how stemming effects, we will build a bag-of-stems for each of the above stemmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a new bag of stems\n",
    "porter_stems = Counter()\n",
    "lancaster_stems = Counter()\n",
    "snowball_stems = Counter()\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, frequency in zip(freq_table[\"word\"], freq_table[\"frequency\"]):\n",
    "    stem = porter_stemmer.stem(word)\n",
    "    if stem in porter_stems:\n",
    "        porter_stems[stem] += frequency\n",
    "    else:\n",
    "        porter_stems[stem] = frequency\n",
    "\n",
    "    stem = lancaster_stemmer.stem(word)\n",
    "    if stem in lancaster_stems:\n",
    "        lancaster_stems[stem] += frequency\n",
    "    else:\n",
    "        lancaster_stems[stem] = frequency\n",
    "\n",
    "    stem = snowball_stemmer.stem(word)\n",
    "    if stem in snowball_stems:\n",
    "        snowball_stems[stem] += frequency\n",
    "    else:\n",
    "        snowball_stems[stem] = frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the stemmers produce similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original: length:\", len(bag), \"common words:\", bag.most_common(15), \"\\n\\n\",)\n",
    "print(\n",
    "    \"Porter: length:\",\n",
    "    len(porter_stems),\n",
    "    \"common stems:\",\n",
    "    porter_stems.most_common(15), \"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"Lancaster: length:\",\n",
    "    len(lancaster_stems),\n",
    "    \"common stems:\",\n",
    "    lancaster_stems.most_common(15), \"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"Snowball: length:\",\n",
    "    len(snowball_stems),\n",
    "    \"common stems:\",\n",
    "    snowball_stems.most_common(15), \"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the largest compression from the Lancaster stemmer.\n",
    "So we use the Lancaster stems to plot the same frequency curve as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table = get_freq_table(lancaster_stems)\n",
    "plt.plot(\n",
    "    np.log(freq_table[\"rank\"]), np.log(freq_table[\"frequency\"]), \".\", markersize=3\n",
    ")\n",
    "\n",
    "plt.title(\"Haiku Stem Frequency\")\n",
    "plt.xlabel(\"$\\log(rank)$\")\n",
    "plt.ylabel(\"$\\log(freq)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the curve does not appear to have changed much from the frequency plot with the stop words removed, except slightly more curved.\n",
    "Perhaps there just aren't that many variants of each word.\n",
    "Or perhaps Zipf's law holds on natural language word stems as well as the words themselves.\n",
    "I think that is more likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization is a more involved process, and takes quite a bit more time.\n",
    "\n",
    "There are two lemmatizers that I will use: one from NLTK, and one from SpaCy.\n",
    "However, the NLTK WordNet lemmatizer supports two modes: with, and without Part-Of-Speech (POS) tagging.\n",
    "\n",
    "So we procede with the three variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table = get_freq_table(bag)\n",
    "\n",
    "wn_lemmas = Counter()\n",
    "wn_pos_lemmas = Counter()\n",
    "spacy_lemmas = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "for word, frequency in zip(freq_table[\"word\"], freq_table[\"frequency\"]):\n",
    "    lemma = lem.lemmatize(word)\n",
    "    if lemma in wn_lemmas:\n",
    "        wn_lemmas[lemma] += frequency\n",
    "    else:\n",
    "        wn_lemmas[lemma] = frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tags = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV,\n",
    "    }\n",
    "    # Default to a noun if the POS is unknown.\n",
    "    return tags.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, frequency in zip(freq_table[\"word\"], freq_table[\"frequency\"]):\n",
    "    lemma = lem.lemmatize(word, get_pos(word))\n",
    "    if lemma in wn_pos_lemmas:\n",
    "        wn_pos_lemmas[lemma] += frequency\n",
    "    else:\n",
    "        wn_pos_lemmas[lemma] = frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that SpaCy lemmatizes tokens in the provided corpus as a part of its model construction, so we build a full model for each of the words in the dataset.\n",
    "This is *not* what SpaCy was designed for, but should we lemmatize the entire dataset, we would not be able associate the lemma's frequency with that of the original word.\n",
    "This is because the bag-of-words is already compressed - there are no duplicate tokens - only an annotation of how frequent each token is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# horrendously slow\n",
    "for word, frequency in zip(freq_table[\"word\"], freq_table[\"frequency\"]):\n",
    "    # This is not what SpaCy was meant for.\n",
    "    doc = _nlp(word)\n",
    "    token = doc[0]\n",
    "    lemma = token.lemma_\n",
    "\n",
    "    if lemma in spacy_lemmas:\n",
    "        spacy_lemmas[lemma] += frequency\n",
    "    else:\n",
    "        spacy_lemmas[lemma] = frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"original: length:\", len(bag), \"most common:\", bag.most_common(15), \"\\n\\n\",)\n",
    "print(\n",
    "    \"WordNet: length:\",\n",
    "    len(wn_lemmas),\n",
    "    \"most common:\",\n",
    "    wn_lemmas.most_common(15), \"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"WordNet with POS: length:\",\n",
    "    len(wn_pos_lemmas),\n",
    "    \"most common:\",\n",
    "    wn_pos_lemmas.most_common(15), \"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"spaCy: length:\",\n",
    "    len(spacy_lemmas),\n",
    "    \"most common:\",\n",
    "    spacy_lemmas.most_common(15), \"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each of the lemmatizers identifies the same most common lemmas, but with different frequencies.\n",
    "The SpaCy lemmatizer does the most compression, so plot the same frequency curve as before using the SpaCy lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table = get_freq_table(spacy_lemmas)\n",
    "\n",
    "plt.plot(\n",
    "    np.log(freq_table[\"rank\"]),\n",
    "    np.log(freq_table[\"frequency\"]),\n",
    "    \".\",\n",
    "    markersize=3,\n",
    ")\n",
    "\n",
    "plt.title(\"Haiku Lemma Frequency\")\n",
    "plt.xlabel(\"$\\log(rank)$\")\n",
    "plt.ylabel(\"$\\log(freq)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern is the same as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "My conclusion is that Zipf's law does in fact hold for haiku.\n",
    "The initial thought was that it might not because haiku are a compressed form of natural language.\n",
    "\n",
    "Interestingly, it holds before and after removing stop words - words like \"an\" and \"the\", which are quite common.\n",
    "Zipf's law is stated abstractly for tokens in a natural language, but holds even for the stems and lemmas of those tokens.\n",
    "This makes sense, and is not surprising."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
