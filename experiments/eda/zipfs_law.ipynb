{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - Zipf's Law\n",
    "\n",
    "* Does zipf's law hold?\n",
    "* Analyze word frequencies before and after removing stop words\n",
    "* Analyze word frequencies before and after stemming/lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport haikulib.data_utils\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipf's Law\n",
    "\n",
    "Simply put, Zipf's law states that the frequencies of words from a natural language corpus are inversely proportional to their rank in a frequency table. That is, a plot of their rank vs frequency on a log-log scale will be roughly linear.\n",
    "\n",
    "For example, The first word in the table below is twice as frequent as the second word, and three times as frequent as the third.\n",
    "\n",
    "| rank | value  | occurrences |\n",
    "|------|--------|-------------|\n",
    "| 1    | word 1 | 21          |\n",
    "| 2    | word 2 | 10          |\n",
    "| 3    | word 3 | 7           |\n",
    "\n",
    "A plot of this frequency table on a log-log scale is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = np.array([1, 2, 3])\n",
    "frequencies = np.array([21, 10, 7])\n",
    "\n",
    "plt.plot(np.log(ranks), np.log(frequencies))\n",
    "plt.plot(np.log(ranks), np.log(frequencies), \".\")\n",
    "\n",
    "plt.title(\"Example of Zipf's Law\")\n",
    "plt.xlabel(\"$\\log(rank)$\")\n",
    "plt.ylabel(\"$\\log(freq)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipf's Law for our Dataset\n",
    "\n",
    "So we get a bag-of-words representation of the dataset, and construct the frequency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_table(bag, thing=\"word\"):\n",
    "    \"\"\"Get a frequency table representation of the given bag-of-<thing> representation.\"\"\"\n",
    "    assert isinstance(bag, Counter)\n",
    "    things, frequencies = zip(*sorted(bag.items(), key=operator.itemgetter(1), reverse=True))\n",
    "    things = np.array(things)\n",
    "    frequencies = np.array(frequencies)\n",
    "    ranks = np.arange(1, len(things) + 1)\n",
    "\n",
    "    freq_table = pd.DataFrame({\"rank\": ranks, thing: things, \"frequency\": frequencies})\n",
    "#     freq_table.set_index(\"rank\", inplace=True, drop=False)\n",
    "    return freq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = haikulib.data_utils.get_bag_of(column=\"haiku\", kind=\"words\")\n",
    "freq_table = get_freq_table(bag)\n",
    "freq_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the ranks of each word vs their frequency on a log-log scale reveals that Zipf's law does seem to hold for most of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(freq_table[\"rank\"]), np.log(freq_table[\"frequency\"]), '.', markersize=3)\n",
    "\n",
    "plt.title(\"Haiku Word Frequency\")\n",
    "plt.xlabel(\"$\\log(rank)$\")\n",
    "plt.ylabel(\"$\\log(freq)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So then we find the words and their corresponding frequencies at the interesting breaks in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(df, column, values):\n",
    "    \"\"\"Gets the indices of values from the given column of the given dataframe.\"\"\"\n",
    "    indices = []\n",
    "    for value in values:\n",
    "        indices += df[column][df[column] == value].index.tolist()\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = get_indices(freq_table, \"word\", [\"the\", \"a\", \"of\", \"to\", \"i\", \"her\", \"his\"])\n",
    "interesting = freq_table.loc[indices]\n",
    "interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    np.log(freq_table[\"rank\"]), np.log(freq_table[\"frequency\"]), \".\", markersize=3\n",
    ")\n",
    "\n",
    "# This should be a crime.\n",
    "x_adjust = np.array([0.1, -0.6, 0.15, -0.6, 0.2, -0.6, 0.0])\n",
    "y_adjust = np.array([1.0, -1.2, 1.0, -1.3, 1.0, -1.3, 1.0])\n",
    "\n",
    "for word, freq, rank, xa, ya in zip(\n",
    "    interesting[\"word\"],\n",
    "    interesting[\"frequency\"],\n",
    "    interesting[\"rank\"],\n",
    "    x_adjust,\n",
    "    y_adjust,\n",
    "):\n",
    "    plt.annotate(\n",
    "        word,\n",
    "        xy=(np.log(rank), np.log(freq) + ya / 20),\n",
    "        xytext=(np.log(rank) + xa, np.log(freq) + ya),\n",
    "        size=9,\n",
    "        arrowprops={\"arrowstyle\": \"-\", \"color\": \"k\"},\n",
    "    )\n",
    "\n",
    "plt.title(\"Haiku Word Frequency\")\n",
    "plt.xlabel(\"$\\log(rank)$\")\n",
    "plt.ylabel(\"$\\log(freq)$\")\n",
    "plt.ylim((-0.5, 11.9))\n",
    "# plt.savefig('zipfs-uncleaned.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipf's Law After Removing Stop Words\n",
    "\n",
    "We remove the stop words from the bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stopword in haikulib.data_utils.STOPWORDS:\n",
    "    if stopword in bag:\n",
    "        del bag[stopword]\n",
    "\n",
    "freq_table = get_freq_table(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    np.log(freq_table[\"rank\"]), np.log(freq_table[\"frequency\"]), \".\", markersize=3\n",
    ")\n",
    "\n",
    "plt.title(\"Haiku Word Frequency\")\n",
    "plt.xlabel(\"$\\log(rank)$\")\n",
    "plt.ylabel(\"$\\log(freq)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table[\"word\"].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = get_indices(freq_table, \"word\", [\"moon\", \"rain\", \"day\", \"night\", \"snow\", \"winter\", \"summer\", \"spring\", \"autumn\"])\n",
    "\n",
    "interesting = freq_table.loc[indices]\n",
    "interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    np.log(freq_table[\"rank\"]), np.log(freq_table[\"frequency\"]), \".\", markersize=3\n",
    ")\n",
    "\n",
    "# This should also be a crime.\n",
    "x_adjust = np.array([-0.35, -0.9, -0.23, -0.9, -0.1, -0.7, 0.3, -0.7, 0.4])\n",
    "y_adjust = np.array([1.0, -1.0, 1.1, -1.1, 1.1, -1.4, 1.0, -1.45, 1.0])\n",
    "\n",
    "for word, freq, rank, xa, ya in zip(\n",
    "    interesting[\"word\"],\n",
    "    interesting[\"frequency\"],\n",
    "    interesting[\"rank\"],\n",
    "    x_adjust,\n",
    "    y_adjust,\n",
    "):\n",
    "    plt.annotate(\n",
    "        word,\n",
    "        xy=(np.log(rank), np.log(freq) + ya / 20),\n",
    "        xytext=(np.log(rank) + xa, np.log(freq) + ya),\n",
    "        size=8,\n",
    "        arrowprops={\"arrowstyle\": \"-\", \"color\": \"k\"},\n",
    "    )\n",
    "\n",
    "plt.title(\"Haiku Word Frequency\")\n",
    "plt.xlabel(\"$\\log(rank)$\")\n",
    "plt.ylabel(\"$\\log(freq)$\")\n",
    "plt.xlim((-0.5, 10.5))\n",
    "plt.ylim((-0.5, 9))\n",
    "# plt.savefig(\"zipfs-cleaned.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequencies After Stemming/Lemmatization\n",
    "\n",
    "There are two approaches for getting the root form of a word. The first is stemming.\n",
    "\n",
    "Stemming involves a sequence of rules used to strip off suffixes of the word to reduce it to its stem - which notably might not be a word. For example, \"leaves\" might be stemmed to form \"leav\". Further, because stemming operates by removing parts of the word, it would fail to stem \"better\" and \"good\" the same.\n",
    "\n",
    "Lemmatization on the other hand, is aware of the vocabulary. It is a more sophisticated technique that returns the word to its base dictionary form via morphological analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = haikulib.data_utils.get_bag_of(column='haiku', kind='words')\n",
    "\n",
    "for stopword in haikulib.data_utils.STOPWORDS:\n",
    "    if stopword in bag:\n",
    "        del bag[stopword]\n",
    "\n",
    "feq_table = get_freq_table(bag)\n",
    "\n",
    "# Build a new bag of stems\n",
    "porter_stems = Counter()\n",
    "lancaster_stems = Counter()\n",
    "snowball_stems = Counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, frequency in zip(freq_table[\"word\"], freq_table[\"frequency\"]):\n",
    "    stem = porter_stemmer.stem(word)\n",
    "    if stem in porter_stems:\n",
    "        porter_stems[stem] += frequency\n",
    "    else:\n",
    "        porter_stems[stem] = frequency\n",
    "\n",
    "    stem = lancaster_stemmer.stem(word)\n",
    "    if stem in lancaster_stems:\n",
    "        lancaster_stems[stem] += frequency\n",
    "    else:\n",
    "        lancaster_stems[stem] = frequency\n",
    "\n",
    "    stem = snowball_stemmer.stem(word)\n",
    "    if stem in snowball_stems:\n",
    "        snowball_stems[stem] += frequency\n",
    "    else:\n",
    "        snowball_stems[stem] = frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the stemmers produce similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original: length:\", len(bag), \"common words:\", bag.most_common(15))\n",
    "print(\n",
    "    \"Porter: length:\",\n",
    "    len(porter_stems),\n",
    "    \"common stems:\",\n",
    "    porter_stems.most_common(15),\n",
    ")\n",
    "print(\n",
    "    \"Lancaster: length:\",\n",
    "    len(lancaster_stems),\n",
    "    \"common stems:\",\n",
    "    lancaster_stems.most_common(15),\n",
    ")\n",
    "print(\n",
    "    \"Snowball: length:\",\n",
    "    len(snowball_stems),\n",
    "    \"common stems:\",\n",
    "    snowball_stems.most_common(15),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the largest compression from the Lancaster stemmer. So we use that to plot the same frequency curve as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table = get_freq_table(lancaster_stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    np.log(freq_table[\"rank\"]), np.log(freq_table[\"frequency\"]), \".\", markersize=3\n",
    ")\n",
    "\n",
    "plt.title(\"Haiku Stem Frequency\")\n",
    "plt.xlabel(\"$\\log(rank)$\")\n",
    "plt.ylabel(\"$\\log(freq)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the curve does not appear to have changed much from the frequency plot with the stop words removed, except slightly more curved. Perhaps there just aren't that many variants of each word. Or perhaps Zipf's law holds on natural language word stems as well as the words themselves. I think that is more likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table = get_freq_table(bag)\n",
    "\n",
    "wn_lemmas = Counter()\n",
    "wn_pos_lemmas = Counter()\n",
    "spacy_lemmas = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "for word, frequency in zip(freq_table[\"word\"], freq_table[\"frequency\"]):\n",
    "    lemma = lem.lemmatize(word)\n",
    "    if lemma in wn_lemmas:\n",
    "        wn_lemmas[lemma] += frequency\n",
    "    else:\n",
    "        wn_lemmas[lemma] = frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tags = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV,\n",
    "    }\n",
    "    # Default to a noun if the POS is unknown.\n",
    "    return tags.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, frequency in zip(freq_table[\"word\"], freq_table[\"frequency\"]):\n",
    "    lemma = lem.lemmatize(word, get_pos(word))\n",
    "    if lemma in wn_pos_lemmas:\n",
    "        wn_pos_lemmas[lemma] += frequency\n",
    "    else:\n",
    "        wn_pos_lemmas[lemma] = frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horrendously slow\n",
    "for word, frequency in zip(freq_table[\"word\"], freq_table[\"frequency\"]):\n",
    "    # This is not what SpaCy was meant for.\n",
    "    doc = nlp(word)\n",
    "    token = doc[0]\n",
    "    lemma = token.lemma_\n",
    "\n",
    "    if lemma in spacy_lemmas:\n",
    "        spacy_lemmas[lemma] += frequency\n",
    "    else:\n",
    "        spacy_lemmas[lemma] = frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"original: length:\", len(bag), \"most common:\", bag.most_common(15))\n",
    "print(\n",
    "    \"WordNet: length:\",\n",
    "    len(wn_lemmas),\n",
    "    \"most common:\",\n",
    "    wn_lemmas.most_common(15),\n",
    ")\n",
    "print(\n",
    "    \"WordNet with POS: length:\",\n",
    "    len(wn_pos_lemmas),\n",
    "    \"most common:\",\n",
    "    wn_pos_lemmas.most_common(15),\n",
    ")\n",
    "print(\n",
    "    \"spaCy: length:\",\n",
    "    len(spacy_lemmas),\n",
    "    \"most common:\",\n",
    "    spacy_lemmas.most_common(15),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each of the lemmatizers identifies the same most common lemmas, but with different frequencies. The spaCy lemmatizer does the most compression, so plot the same frequency curve as before using the spaCy lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table = get_freq_table(spacy_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    np.log(freq_table[\"rank\"]),\n",
    "    np.log(freq_table[\"frequency\"]),\n",
    "    \".\",\n",
    "    markersize=3,\n",
    ")\n",
    "\n",
    "plt.title(\"Haiku Lemma Frequency\")\n",
    "plt.xlabel(\"$\\log(rank)$\")\n",
    "plt.ylabel(\"$\\log(freq)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern is the same as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "My conclusion is that Zipf's law does in fact hold for Haikus. The initial thought was that it might not because haikus are a compressed form of communication. Interestingly, it holds before and after removing stop words - words like \"an\" and \"the\", which are quite common.\n",
    "\n",
    "Zipfs law is stated for tokens in a natural language, but holds even for the stems and lemmas for those tokens. This makes sense, and is not surprising."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
