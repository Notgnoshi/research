{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\author{Austin Gill}\n",
    "\\title{Data Analysis -- Haiku Graph Similarity}\n",
    "\\maketitle\n",
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get an intuitive feeling for how well (how creative) a haiku generator is, we compare the similarity of generated haiku with each of the haiku in the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automagically reimport haikulib if it changes.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "import grakel\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from haikulib import data, nlp, utils\n",
    "\n",
    "data_dir = data.get_data_dir() / \"experiments\" / \"similarity\"\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "pd.set_option(\"display.latex.repr\", True)\n",
    "pd.set_option(\"display.latex.longtable\", True)\n",
    "plt.rcParams[\"figure.figsize\"] = (16 * 0.6, 9 * 0.6)\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generated_df():\n",
    "    return pd.read_csv(\n",
    "        # TODO: Actually generate this CSV file.\n",
    "        data.get_data_dir() / \"experiments\" / \"generation\" / \"knesser-ney\" / \"generated.csv\",\n",
    "        index_col=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corpus = data.get_df()\n",
    "# Exceedingly slow\n",
    "corpus[\"lemma\"] = list(nlp.lemmatize(corpus[\"haiku\"]))\n",
    "generated = get_generated_df()\n",
    "generated[\"lemma\"] = list(nlp.lemmatize(generated[\"haiku\"]))\n",
    "generated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haiku2edges(haiku):\n",
    "    edges = collections.Counter()\n",
    "    tokens = nltk.word_tokenize(haiku)\n",
    "    edges.update(utils.pairwise(tokens))\n",
    "    return edges\n",
    "\n",
    "def edges2grakel(edges):\n",
    "    tokens = set(itertools.chain.from_iterable(edges))\n",
    "    return grakel.Graph(edges, node_labels={k:k for k in tokens})\n",
    "\n",
    "def edges2nx(edges):\n",
    "    graph = nx.DiGraph()\n",
    "    for edge, weight in edges.items():\n",
    "        graph.add_edge(*edge, weight=weight)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gen_edges = [haiku2edges(h) for h in generated[\"lemma\"]]\n",
    "gen_graphs = [edges2grakel(e) for e in gen_edges]\n",
    "gen_nx_graphs = [edges2nx(e) for e in gen_edges]\n",
    "\n",
    "corpus_edges = (haiku2edges(h) for h in corpus[\"lemma\"])\n",
    "corpus_graphs = [edges2grakel(e) for e in corpus_edges]\n",
    "\n",
    "# There are enough haiku in the dataset that we need to use sparse representations.\n",
    "graph_kernel = grakel.kernels.WeisfeilerLehman(n_iter=2, normalize=True, base_kernel=(grakel.kernels.VertexHistogram, {\"sparse\": True}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = gen_nx_graphs[1]\n",
    "pos = nx.spring_layout(graph)\n",
    "nx.draw(graph, pos, with_labels=True, node_size=900)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for query_graph, query in zip(gen_graphs, generated[\"haiku\"]):\n",
    "    graph_kernel.fit([query_graph])\n",
    "    kernel = graph_kernel.transform(corpus_graphs)\n",
    "    \n",
    "    # number of similar haiku to find\n",
    "    n = 3\n",
    "    indices = np.argsort(kernel[:, 0])[-n:]\n",
    "    similar = corpus.iloc[indices]\n",
    "    print(\"query:\", query)\n",
    "    for sim in similar[\"haiku\"]:\n",
    "        print(\"\\tsimilar:\", sim)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
