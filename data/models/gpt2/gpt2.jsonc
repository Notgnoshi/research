{
    "type": "transformer",
    // If type is "transformer", specify which model to use. Options: "gpt", "gpt2"
    "model_type": "gpt2",
    // Path to save any generated haiku relative to this file.
    "generated_path": "../../generated.csv",
    // A uint32_t random seed.
    "seed": null,
    // Whether to add the ^...$ tags surrounding each haiku (recommended)
    "tags": true,
    "max_tokens": 20,
    // The prompt to use to generate haiku.
    "prompt": "^ cherry",
    // The number of haiku to generate with the above prompt.
    "number": 10,
    // Batch size per GPU/CPU.
    "batch_size": 4,
    // Number of update steps to accumulate before performings a backward/update pass.
    "gradient_accumulation_steps": 1,
    "learning_rate": 5e-5,
    "weight_decay": 0.0,
    "adam_epsilon": 1e-8,
    "max_gradient_norm": 1.0,
    "num_train_epochs": 1,
    // Maximum number of training steps to perform, if set. Overrides "num_train_epochs"
    "max_steps": 105,
    // Use a linear warmpup over n steps.
    "warmup_steps": 0,
    // Proportion of the training set to use as evaluation data.
    "evaluation_proportion": 0.1,
    // Run model evaluation at each logging step.
    "evaluate_during_training": true,
    // Log every n steps.
    "logging_steps": 500,
    // Save a restorable checkpoint ever n steps.
    "checkpoint_steps": 500,
    // Restore and resume training from the given checkpoint, if given.
    // Path to the checkpoint directory is relative to this file.
    "resume_training_from": null,
    // Number of checkpoints to save. Oldest deleted first.
    "max_checkpoints": null,
    // Whether or not to use CUDA. The GPT-2 model doesn't fit on a GTX 1080.
    "cuda": false
}
