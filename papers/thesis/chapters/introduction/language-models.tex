% !TeX root = .//.tex
\section{Modeling Natural Language}\label{sec:introduction:language-models}

Natural language in its textual form may be represented as a sequence of tokens.
The specific granularity of these token sequences varies from the coarsest word level, to individual characters, to different variations of subword morphemes, token metadata tags (like its grammatical part of speech), and punctuation.
A Language Model (LM) is a statistical model of natural language that generates a probability distribution for these sequences \cite{pappas_meyer_2012,goldberg_2017}.

In particular, given a sequence of words $w_{1:n}$, we wish to estimate the probability $P(w_{1:n})$. In the general case, we can use the probability chain rule
\begin{equation}
    P(w_{1:n}) = P(w_n \mid w_{1:n - 1}) \cdot P(w_{n - 1} \mid w_{1:n - 2}) \cdots P(w_2 \mid w_1) \cdot P(w_1)\label{eq:language-models:chain-rule}
\end{equation}
to confirm our intuition that a language model's understanding of a given word in a sequence relies on an understanding of the full context from the current word all the back to the first word in the sequence.
That is, understanding future tokens in a sequence requires understanding not only the present, but also the entire past history of the sequence.

\subsection{Markov Models}\label{sec:language-models:markov}

A simple approach to language modeling, useful for motivating the rest of our discussion, is to use a Markov chain to model the probability that one word follows another in a given sequence.
A key property of Markov models is that they are stateless, or memoryless \cite{gagniuc_2017}.
That is, that the probability of a transition to a future state depends only on the current state, and not on the history of previous states.
\begin{equation}
    P(w_{i + 1} \mid w_{1:i}) \approx P(w_{i+1} \mid w_i) \label{eq:language-models:markov-chain}
\end{equation}
This means that, as stated, Markov chain models are unsuited to modeling textual data due to their lack of observance of the large amount of context needed to understand natural language.

Traditional approaches to building language models relaxes the general requirement of retaining the full history of a sequence \cite{mikolov2012statistical}, but not as far as the Markov chain model presented in \autoref{eq:language-models:markov-chain}.
These traditional approaches assume the $k$th order Markov property \cite{goldberg_2017} --- that the next word in a sequence depends on only the last $k$ words of the sequence instead of the full $n$.
\begin{equation}
    P(w_{i+1} \mid w_{1:i}) \approx P(w_{i+1} \mid w_{i:i-k}) \label{eq:language-models:k-order-markov}
\end{equation}
Under this assumption, we can estimate a sequence's probability as
\begin{equation}
    P(w_{1:n}) \approx \prod_{i=1}^n P(w_i \mid w_{i-k:i-1}) \label{eq:language-models:k-order-sequence-probability}
\end{equation}

One method of producing this estimate is to use the maximum likelihood estimate (MLE)
\begin{equation}
    \hat p (w_{i+1} \mid w_{i-k:i}) = \frac{\countf{w_{i-k:i+1}}}{\countf{w_{i-k:i}}} \label{eq:language-models:mle}
\end{equation}
for each subsequence $w_{i-k:i}$ in the corpus.

However, one limitation of this approach is its lack of creative capacity.
\sarcasm{Similarly, one limitation of this thesis is its lack of creativity.}
That is, if a subsequence $w_{i-k:i+1}$ was never observed in the corpus, then its estimated probability is zero \cite{goldberg_2017}.

At first, this does not seem to pose a problem.
If a sequence was not observed in a training corpus, we ought not expect a language model that understands that sequence.
However, due to the compositional nature of natural language, it is likely that there are many more sequences that make sense than there are sequences in the training corpus.
Thus, if we want an understanding of natural language as a whole we must be able to extrapolate meaning from sequence we have never seen before.

There are several approaches to avoiding these zero events.
One family of approaches is called \textit{smoothing}, where every possible sequence is provided some small probability mass.
An example of smoothing is called \textit{additive smoothing} \cite{chen_goodman_1999,goodman_2001} where zero probabilities are avoided by assuming that each event occurs at least $\alpha$ times in addition to its observed occurrences in the corpus.
The MLE estimate is modified as
\begin{equation}
    \hat p (w_{i+1} \mid w_{i-k:i}) = \frac{\countf{w_{i-k:i+1}} + \alpha}{\countf{w_{i-k:i}} + \alpha v} \label{eq:language-models:alpha-smoothing}
\end{equation}
where $v$ is the size of the token vocabulary and $0 < \alpha \leq 1$. Another family of approaches is using \textit{back-off}, where if a sequence $w_{i-k:i}$ is not observed, the model falls back to using $w_{i-k-1:i}$\cite{chen_goodman_1999,goodman_2001,JelMer80}.

\subsection{Feed-Forward Neural Network Models}\label{sec:language-models:ffnns}
Using feed-forward neural networks were first seriously applied to modeling natural language by Yoshua Bengio in 2003 \cite{bengio2003}.
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[
            node distance=2.6cm,
            >=stealth,
            layer/.style={draw, minimum width=1.5cm},
            word_vect/.style={draw, dashed, minimum height=2cm}
        ]

        \node[word_vect] (w1) {$C(w_1)$};
        \node[word_vect, below=1mm of w1, anchor=north] (w2) {$C(w_2)$};

        \node[below=3mm of w2, anchor=north] (ellipsis) {$\vdots$};

        \node[word_vect, below= 3mm of ellipsis, anchor=north] (wk) {$C(w_k)$};

        \node[left=1cm of w1] (w1_label) {$w_1$};
        \node[left=1cm of w2] (w2_label) {$w_2$};
        \node[left=1cm of wk] (wk_label) {$w_k$};

        \node[layer, fit={(w1) (w2) (ellipsis) (wk)}, label=below:{$\vec x \in \R^{km}$}] (input) {};

        \node[layer, label=below:{$\tanh$}, right of=input, minimum height=5cm] (hidden) {};
        \node[layer, label=below:{softmax}, right of=hidden, minimum height=10cm] (softmax) {};
        \node[right=1.2cm of softmax] (output) {$P(w_{k+1} \mid w_{1:k})$};

        \draw[->, shorten >=1mm] (w1_label) -- (w1);
        \draw[->, shorten >=1mm] (w2_label) -- (w2);
        \draw[->, shorten >=1mm] (wk_label) -- (wk);

        \draw[->] (input) -- (hidden);
        \draw[->] (hidden) -- (softmax);
        \draw[->] (softmax) -- (output);
    \end{tikzpicture}
    \caption{Bengio's FFNNLM architecture}\label{fig:language-models:ffnns}
\end{figure}
Bengio used a feed-forward network, where each word in the vocabulary is mapped to a $m$-dimensional vector in a continuous vector space.
\sarcasm{The paper~\cite{bengio2003} is light on the details of the mapping $C : V \to \R^m$.
    \cite{pappas_meyer_2012} fills in some of the details, but still doesn't explain how $C$ is constructed.
    It seems to indicate that $C$ is \textit{learned} at the same time as the rest of the network?
    So does that means that the actual inputs are the word indices?}
Then each word in the sequence $w_{i-k:i}$ is mapped to their corresponding vectors, which are then concatenated to form the $k \cdot m$-dimensional input vector for the neural network.

The softmax output layer shown in \autoref{fig:language-models:ffnns} is the most computational aspect of Bengio's language model ---
given a vocabulary of size $v$, one softmax computation requires a matrix-vector multiplication with a matrix sized $d_\text{hidden} \times v$, followed by $v$ exponentiations.
This makes using large vocabularies prohibitively expensive~\cite{goldberg_2017}.

\subsection{Recurrent Neural Network Models}\label{sec:language-models:rnns}

\TODO{See \url{https://arxiv.org/pdf/1906.03591.pdf} and \url{http://publications.idiap.ch/downloads/reports/2012/Pappas_Idiap-RR-32-2012.pdf}, and find the Mikolov source.}

\subsection{Long Short Term Memory Models}\label{sec:language-models:lstms}

\TODO{See \url{https://dspace5.zcu.cz/bitstream/11025/16981/1/SoutnerD_2013_ApplicationofLSTM.pdf}}

\subsection{Word Embeddings}\label{sec:language-models:word-embeddings}
\subsection{word2vec}\label{sec:language-models:word2vec}
\subsection{Global Vectors for Word Representation}\label{sec:language-models:glove}
\subsection{doc2vec}\label{sec:language-models:doc2vec}

\subsection{Encoders and Decoders}\label{sec:language-models:encoders-decoders}
\subsection{seq2seq}\label{sec:language-models:seq2seq}

\subsection{Attention}\label{sec:language-models:attention}
\subsection{Transformers}\label{sec:language-models:transformers}
\subsection{Byte Pair Encodings}\label{sec:language-models:bpe}
\subsection{Bidirectional Encoder Representations from Transformers}\label{sec:language-models:bert}
\subsection{Generative Pre-Training (I)}\label{sec:language-models:gpt}
\subsection{Generative Pre-Training (II)}\label{sec:language-models:gpt-2}
