% !TeX root = ../introduction.tex
\section{Modeling Natural Language}\label{sec:introduction:language-models}

Natural language in its textual form may be represented as a sequence of tokens.
The specific granularity of these token sequences varies from the coarsest word level, to individual characters, to different variations of subword morphemes, token metadata tags (like its grammatical part of speech), and punctuation.
A Language Model (LM) is a statistical model of natural language that generates a probability distribution for these sequences \cite{pappas_meyer_2012,goldberg_2017}.

In particular, given a sequence of words $w_{1:n}$, we wish to estimate the probability $P(w_{1:n})$.
In the general case, we can use the probability chain rule
\begin{equation}
    P(w_{1:n}) = P(w_n \mid w_{1:n - 1}) \cdot P(w_{n - 1} \mid w_{1:n - 2}) \cdots P(w_2 \mid w_1) \cdot P(w_1)\label{eq:language-models:chain-rule}
\end{equation}
to confirm our intuition that a language model's understanding of a given word in a sequence relies on an understanding of the full context from the current word all the back to the first word in the sequence.
That is, understanding future tokens in a sequence requires understanding not only the present, but also the entire past history of the sequence.

Of course, with any probabilistic model, we must have a method of scoring the model.\sarcasm{The best intuitive explanations of perplexity and its relation to entropy are \url{https://leimao.github.io/blog/Entropy-Perplexity/} and \url{https://stats.stackexchange.com/questions/10302/what-is-perplexity}}
In Shannon's seminal work on information theory \cite{Shannon1948} he discussed modeling an information source inherent information using entropy.
In the discrete case, \textit{Shannon entropy} is defined as
\begin{equation}
    \entropy(p) = - \sum_{i=1}^n p(x_i) \log_b p(x_i) \label{eq:language-models:entropy}
\end{equation}
where $p(x_i)$ is the probability of state $x_i$, and $\sum_i p(x_i) = 1$.
Entropy can be interpreted as the information content of the modeled information source.

Then the \textit{cross entropy} of two distributions $p$ and $q$ can be defined in the discrete case as
\begin{equation}
    \entropy(p, q) =  - \sum_{i=1}^n p(x_i) \log_b q(x_i) \label{eq:language-models:cross-entropy}
\end{equation}
\CITE{Cross entropy can be thought of as a measure of the difference between two probability distributions.}
Thus, cross entropy is often used as an objective function in optimization tasks, because we want to minimize the difference between the probability distribution of the training set (as an estimate of the \textit{true} probability distribution of the whole), and the distribution of the statistical model's output.

\CITE{\textit{Perplexity} is defined as}
\begin{equation}
    \perplexity(p) = b^{- \sum_{i=1}^n p(x_i) \log_b p(x_i)} \label{eq:language-models:perplexity-defn}
\end{equation}
which is precisely $b^{\entropy(p)}$, or the exponentiation of Shannon entropy!

However, when scoring a language model, we rarely (if ever) know the true probability distribution of the language a dataset it sampled from.
So we modify \autoref{eq:language-models:perplexity-defn} to approximate the perplexity of a language model from the training set probability distribution $\tilde p$, and the language model's output distribution
\begin{equation}
    \widetilde{\perplexity}(\tilde p, \model)  = b^{- \sum_{i=1}^{n} \tilde{p}(w_i) \log_b \model(w_i)}\label{eq:language-models:perplexity-p-model}
\end{equation}
where our training dataset is formed of $n$ words $w_1, \dots, w_n$.
But the word $w_i$ is sampled uniformly from the dataset, so $\tilde p(w_i) = \frac{1}{n}$, which results in the definition of perplexity that \cite{goldberg_2017} gives.
\begin{equation}
    \perplexity(\model) = 2^{-\frac{1}{n}\sum_{i=1}^n \log_2 \model(w_i \mid w_{1:i-1})} \label{eq:language-models:perplexity}
\end{equation}

Notice that the worst possible language model would be a random choice of the next token in a sequence with a uniform distribution.
Such a model would have each $\model(w_i \mid w_{1:i-1}) = \frac{1}{n}$, which would result in a high perplexity score.
A good language model --- one that is reflective of ``real'' language --- will assign high probabilities to observed events in the test corpus, which results in minimizing the perplexity score \cite{goldberg_2017}.
Notice, however, that perplexities are corpus specific --- rendering it impossible to compare scores between two language models trained on different datasets.

\subsection{Markov Models}\label{sec:language-models:markov}

A simple approach to language modeling, useful for motivating the rest of our discussion, is to use a Markov chain to model the probability that one word follows another in a given sequence.
A key property of Markov models is that they are stateless, or memoryless \cite{gagniuc_2017}.
That is, that the probability of a transition to a future state depends only on the current state, and not on the history of previous states.
\begin{equation}
    P(w_{i + 1} \mid w_{1:i}) \approx P(w_{i+1} \mid w_i) \label{eq:language-models:markov-chain}
\end{equation}
This means that, as stated, Markov chain models are unsuited to modeling textual data due to their lack of observance of the large amount of context needed to understand natural language.

Traditional approaches to building language models relaxes the general requirement of retaining the full history of a sequence \cite{mikolov2012statistical}, but not as far as the Markov chain model presented in \autoref{eq:language-models:markov-chain}.
These traditional approaches assume the $k$th order Markov property \cite{goldberg_2017} --- that the next word in a sequence depends on only the last $k$ words of the sequence instead of the full $n$.
\begin{equation}
    P(w_{i+1} \mid w_{1:i}) \approx P(w_{i+1} \mid w_{i:i-k}) \label{eq:language-models:k-order-markov}
\end{equation}
Under this assumption, we can estimate a sequence's probability as
\begin{equation}
    P(w_{1:n}) \approx \prod_{i=1}^n P(w_i \mid w_{i-k:i-1}) \label{eq:language-models:k-order-sequence-probability}
\end{equation}

One method of producing this estimate is to use the maximum likelihood estimate (MLE)
\begin{equation}
    \hat p (w_{i+1} \mid w_{i-k:i}) = \frac{\countf{w_{i-k:i+1}}}{\countf{w_{i-k:i}}} \label{eq:language-models:mle}
\end{equation}
for each subsequence $w_{i-k:i}$ in the corpus.

However, one limitation of this approach is its lack of creative capacity.
\sarcasm{Similarly, one limitation of this thesis is its lack of creativity.}
That is, if a subsequence $w_{i-k:i+1}$ was never observed in the corpus, then its estimated probability is zero \cite{goldberg_2017}.

At first, this does not seem to pose a problem.
If a sequence was not observed in a training corpus, we ought not expect a language model that understands that sequence.
However, due to the compositional nature of natural language, it is likely that there are many more sequences that make sense than there are sequences in the training corpus.
Thus, if we want an understanding of natural language as a whole we must be able to extrapolate meaning from sequence we have never seen before.

There are several approaches to avoiding these zero events.
One family of approaches is called \textit{smoothing}, where every possible sequence is provided some small probability mass.
An example of smoothing is called \textit{additive smoothing} \cite{chen_goodman_1999,goodman_2001} where zero probabilities are avoided by assuming that each event occurs at least $\alpha$ times in addition to its observed occurrences in the corpus.
The MLE estimate is modified as
\begin{equation}
    \hat p (w_{i+1} \mid w_{i-k:i}) = \frac{\countf{w_{i-k:i+1}} + \alpha}{\countf{w_{i-k:i}} + \alpha v} \label{eq:language-models:alpha-smoothing}
\end{equation}
where $v$ is the size of the token vocabulary and $0 < \alpha \leq 1$. Another family of approaches is using \textit{back-off}, where if a sequence $w_{i-k:i}$ is not observed, the model falls back to using $w_{i-k-1:i}$\cite{chen_goodman_1999,goodman_2001,JelMer80}.

\subsection{Feed-Forward Neural Network Models}\label{sec:language-models:ffnns}
Using feed-forward neural networks were first seriously applied to modeling natural language by Yoshua Bengio in 2003 \cite{bengio2003}.
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[
        node distance=2.6cm,
        >={Stealth[length=2mm]},
        layer/.style={draw, minimum width=1.5cm},
        word_vect/.style={draw, dashed, minimum height=2cm}
        ]

        \node[word_vect] (w1) {$C(w_1)$};
        \node[word_vect, below=1mm of w1, anchor=north] (w2) {$C(w_2)$};

        \node[below=3mm of w2, anchor=north] (ellipsis) {$\vdots$};

        \node[word_vect, below= 3mm of ellipsis, anchor=north] (wk) {$C(w_k)$};

        \node[left=1cm of w1] (w1_label) {$w_1$};
        \node[left=1cm of w2] (w2_label) {$w_2$};
        \node[left=1cm of wk] (wk_label) {$w_k$};

        \node[layer, fit={(w1) (w2) (ellipsis) (wk)}, label=below:{$\vec x \in \R^{km}$}] (input) {};

        \node[layer, label=below:{$\tanh$}, right of=input, minimum height=5cm] (hidden) {};
        \node[layer, label=below:{softmax}, right of=hidden, minimum height=10cm] (softmax) {};
        \node[right=1.2cm of softmax] (output) {$P(w_{k+1} \mid w_{1:k})$};

        \draw[->] (w1_label) -- (w1);
        \draw[->] (w2_label) -- (w2);
        \draw[->] (wk_label) -- (wk);

        \draw[->] (input) -- (hidden);
        \draw[->] (hidden) -- (softmax);
        \draw[->] (softmax) -- (output);
    \end{tikzpicture}
    \caption{Bengio's feed-forward language model architecture}\label{fig:language-models:ffnnlm}
\end{figure}
Bengio used a feed-forward network, where each word in the vocabulary is mapped to a $m$-dimensional vector in a continuous vector space.
\sarcasm{The paper~\cite{bengio2003} is light on the details of the mapping $C : V \to \R^m$.
    \cite{pappas_meyer_2012} fills in some of the details, but still doesn't explain how $C$ is constructed.
    It seems to indicate that $C$ is \textit{learned} at the same time as the rest of the network?
    So does that means that the actual inputs are the word indices?}
Then each word in the sequence $w_{i-k:i}$ is mapped to their corresponding vectors, which are then concatenated to form the $k \cdot m$-dimensional input vector for the neural network.

The softmax output layer shown in \autoref{fig:language-models:ffnnlm} is the most computational aspect of Bengio's language model ---
given a vocabulary of size $v$, one softmax computation requires a matrix-vector multiplication with a matrix sized $d_\text{hidden} \times v$, followed by $v$ exponentiations.
This makes using large vocabularies prohibitively expensive~\cite{goldberg_2017}.

\subsection{Recurrent Neural Network Models}\label{sec:language-models:rnns}

For several years, Bengio's feed-forward language model shown in \autoref{fig:language-models:ffnnlm} was the state-of-the-art neural network language model.
Then in 2010, Tomas Mikolov~\cite{Mikolov2010RecurrentNN,Mikolov2011RecurrentNN,Mikolov2012ContextDR} proposed a recurrent neural network (RNN) architecture that made major advancements in model performance.

The use of a recurrent network over the standard feed-forward models has several advantages~\cite{goldberg_2017}.
Importantly, they allow representing arbitrary-length sequences as fixed-size vectors.
This is of particular importance in modeling natural language, where the lengths of sentences are quite varied.
RNNs also abandon the $k$-th order Markov assumption in \autoref{eq:language-models:k-order-sequence-probability} that Bengio's FFNN language model follows.
The use of recurrent networks allows the output probabilities of a neural network language model to be conditioned on the \textit{entire} preceding context, instead of a fixed $k$ number of preceding tokens.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[
        node distance=3cm,
        >={Stealth[length=2mm]},
        layer/.style={draw, minimum width=1.8cm},
        ]
        \node[draw, dashed, minimum width=1.8cm, minimum height=6cm] (input) {$x(t)$};
        \node[draw, dashed, minimum width=1.8cm, minimum height=3cm, below=3mm of input] (prev_context) {$C(t-1)$};

        \node[layer, fit={(input) (prev_context)}] (input_layer) {};
        \node[layer, minimum height=3cm, right of=input_layer, label=below:{sigmoid}] (context) {$C(t)$};
        \node[layer, minimum height=8cm, right of=context, label=below:{softmax}] (softmax) {$y(t)$};

        \node[left=1.2cm of input] (input_label) {$w(t)$};
        \node[right=1.2cm of softmax] (output_label) {$P(w(t+1) \mid w(t), C(t-1))$};

        \draw[->] (input_label) -- (input);
        \draw[->] (input_layer) -- (context);
        \draw[->] (context) -- (softmax);
        \draw[->] (context.east) to [out=350, in=0, looseness=1.6] (prev_context);
        \draw[->] (softmax) -- (output_label);
    \end{tikzpicture}
    \caption{Mikolov's recurrent language model architecture}\label{fig:language-models:rnnlm}
\end{figure}

Mikolov's architecture is shown in \autoref{fig:language-models:rnnlm}.
The word $W(t)$ is converted to a one-hot encoding\footnote{
    One-hot encoding is a strategy that uses a boolean vector with an index for each word in the vocabulary.
    To represent the word $w$ from vocabulary $V$ with index $i$ in a one-hot encoded vector $x$, we set $x_i = 1$, and every other index is set to zero.}
$x(t)$, which is then concatenated with the context $C(t-1)$ from the previous time step\footnote{
    As with all recurrence relations, care should be taken with the initial conditions.
    Mikolov set $C(0)$ to a vector of small real values close to 0.1.},
and used as the input to a hidden context layer.
The context layer is then fed to the next iteration's input and the current iteration's softmax output layer to output a probability distribution for the next word $W(t+1)$.
Mikolov notes that the size of the hidden context layer should be proportional to the size of the data set.
This results in training times that increase more than linearly with the size of the training set.

\subsection{Long Short Term Memory Models}\label{sec:language-models:lstms}

\TODO{See \url{https://dspace5.zcu.cz/bitstream/11025/16981/1/SoutnerD_2013_ApplicationofLSTM.pdf}}

\subsection{Word Embeddings}\label{sec:language-models:word-embeddings}
\subsection{word2vec}\label{sec:language-models:word2vec}
\subsection{Global Vectors for Word Representation}\label{sec:language-models:glove}
\subsection{doc2vec}\label{sec:language-models:doc2vec}

\subsection{Encoders and Decoders}\label{sec:language-models:encoders-decoders}
\subsection{seq2seq}\label{sec:language-models:seq2seq}

\subsection{Attention}\label{sec:language-models:attention}
\subsection{Transformers}\label{sec:language-models:transformers}
\subsection{Byte Pair Encodings}\label{sec:language-models:bpe}
\subsection{Bidirectional Encoder Representations from Transformers}\label{sec:language-models:bert}
\subsection{Generative Pre-Training (I)}\label{sec:language-models:gpt}
\subsection{Generative Pre-Training (II)}\label{sec:language-models:gpt-2}
