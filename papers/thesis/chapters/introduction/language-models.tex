% !TeX root = ../introduction.tex
\section{Modeling Natural Language}\label{sec:introduction:language-models}

Natural language in its textual form may be represented as a sequence of tokens.
The specific granularity of the tokens in these sequences varies from the coarsest word level, to individual characters, to different variations of subword morphemes, token metadata tags, and punctuation.
A Language Model (LM) is a statistical model of natural language that generates a probability distribution for these sequences \cite{goldberg_2017, pappas_meyer_2012}.

In particular, given a sequence of words $w_{1:n}$, we wish to estimate the probability $P(w_{1:n})$.
In the general case, we can use the probability chain rule
\begin{equation}
    P(w_{1:n}) = P(w_n \mid w_{1:n - 1}) \cdot P(w_{n - 1} \mid w_{1:n - 2}) \cdots P(w_2 \mid w_1) \cdot P(w_1)\label{eq:language-models:chain-rule}
\end{equation}
to confirm our intuition that a language model's understanding of a given word in a sequence relies on an understanding of the full context from the current word all the back to the first word in the sequence.
That is, understanding future tokens in a sequence requires understanding not only the present, but also the entire past history of the sequence.

Of course, with any probabilistic model, we must have a method of scoring the model.\sarcasm{
    The best intuitive explanations of perplexity and its relation to entropy are \url{https://leimao.github.io/blog/Entropy-Perplexity/} and \url{https://stats.stackexchange.com/questions/10302/what-is-perplexity}}\sarcasm{
    Footnotes marked with roman numerals are for personal remarks by the author, and will be removed before submission.
}
In Shannon's seminal work on information theory he discussed modeling an information source inherent information using entropy \cite{Shannon1948}.
In the discrete case, \textit{Shannon entropy} is defined as
\begin{equation}
    \entropy(p) = - \sum_{i=1}^n p(x_i) \log_b p(x_i) \label{eq:language-models:entropy}
\end{equation}
where $p(x_i)$ is the probability of state $x_i$, and $\sum_i p(x_i) = 1$.
Entropy can be interpreted as the information content of the modeled information source.

Then the \textit{cross entropy} of two distributions $p$ and $q$ can be defined in the discrete case as
\begin{equation}
    \entropy(p, q) =  - \sum_{i=1}^n p(x_i) \log_b q(x_i) \label{eq:language-models:cross-entropy}
\end{equation}
Cross entropy can be thought of as a measure of the difference between two probability distributions \cite{ManningSchuetze99}.
Thus, cross entropy is often used as an objective function in optimization tasks, because we want to minimize the difference between the probability distribution of the training set (as an estimate of the \textit{true} probability distribution of the whole), and the distribution of the statistical model's output.

\textit{Perplexity} is defined as
\begin{equation}
    \perplexity(p) = b^{- \sum_{i=1}^n p(x_i) \log_b p(x_i)} \label{eq:language-models:perplexity-defn}
\end{equation}
which is precisely $b^{\entropy(p)}$, or the exponentiation of Shannon entropy \cite{ManningSchuetze99}!\sarcasm{
    Due to its similarity, \cite{ManningSchuetze99} supposes that perplexity is preferred over cross-entropy as a performance metric due to the more impressive nature of being able to claim large reductions in perplexity as opposed to a cross-entropy loss of a fraction of a bit.}

However, when scoring a language model, we rarely (if ever) know the true probability distribution of the language a dataset it sampled from.
So we modify \autoref{eq:language-models:perplexity-defn} to approximate the perplexity of a language model from the training set probability distribution $\tilde p$, and the language model's output distribution
\begin{equation}
    \widetilde{\perplexity}(\tilde p, \model)  = b^{- \sum_{i=1}^{n} \tilde{p}(w_i) \log_b \model(w_i)}\label{eq:language-models:perplexity-p-model}
\end{equation}
where our training dataset is formed of $n$ words $w_1, \dots, w_n$.
But the word $w_i$ is sampled uniformly from the dataset, so $\tilde p(w_i) = \frac{1}{n}$, which results in the definition of perplexity that \cite{goldberg_2017} gives.
\begin{equation}
    \perplexity(\model) = 2^{-\frac{1}{n}\sum_{i=1}^n \log_2 \model(w_i \mid w_{1:i-1})} \label{eq:language-models:perplexity}
\end{equation}

Notice that the worst possible language model would be a random choice of the next token in a sequence with a uniform distribution.
Such a model would have each $\model(w_i \mid w_{1:i-1}) = \frac{1}{n}$, which would result in a high perplexity score.
A good language model --- one that is reflective of ``real'' language --- will assign high probabilities to observed events in the test corpus, which results in minimizing the perplexity score \cite{goldberg_2017}.
Notice, however, that perplexities are corpus specific --- rendering it impossible to compare scores between two language models trained on different datasets.
\TODO{Provide example language model output and compare perplexities? Possibly just give an example of ``I'm going out to dinner with my...''.}

\subsection{Markov Models}\label{sec:language-models:markov}

A simple approach to language modeling, useful for motivating the rest of our discussion, is to use a Markov chain to model the probability that one word follows another in a given sequence.
A key property of Markov models is that they are stateless, or memoryless \cite{gagniuc_2017}.
That is, that the probability of a transition to a future state depends only on the current state, and not on the history of previous states.
\begin{equation}
    P(w_{i + 1} \mid w_{1:i}) \approx P(w_{i+1} \mid w_i) \label{eq:language-models:markov-chain}
\end{equation}
This means that, as stated, Markov chain models are unsuited to modeling textual data due to their lack of observance of the large amount of context needed to understand natural language.

Traditional approaches to building language models relaxes the general requirement of retaining the full history of a sequence, but not as far as the Markov chain model presented in \autoref{eq:language-models:markov-chain} \cite{mikolov2012statistical}.
These traditional approaches assume the $k$th order Markov property --- that the next word in a sequence depends on only the last $k$ words of the sequence instead of the full $n$ \cite{goldberg_2017}.
\begin{equation}
    P(w_{i+1} \mid w_{1:i}) \approx P(w_{i+1} \mid w_{i:i-k}) \label{eq:language-models:k-order-markov}
\end{equation}
Under this assumption, we can estimate a sequence's probability as
\begin{equation}
    P(w_{1:n}) \approx \prod_{i=1}^n P(w_i \mid w_{i-k:i-1}) \label{eq:language-models:k-order-sequence-probability}
\end{equation}

One method of producing this estimate is to use the maximum likelihood estimate (MLE)
\begin{equation}
    \hat p (w_{i+1} \mid w_{i-k:i}) = \frac{\countf{w_{i-k:i+1}}}{\countf{w_{i-k:i}}} \label{eq:language-models:mle}
\end{equation}
for each subsequence $w_{i-k:i}$ in the corpus.

However, one limitation of this approach is its lack of creative capacity.
\sarcasm{Similarly, one limitation of this thesis is its lack of creativity.}
That is, if a subsequence $w_{i-k:i+1}$ was never observed in the corpus, then its estimated probability is zero \cite{goldberg_2017}.

At first, this does not seem to pose a problem.
If a sequence was not observed in a training corpus, we ought not expect a language model that understands that sequence.
However, due to the compositional nature of natural language, it is likely that there are many more sequences that make sense than there are sequences in the training corpus.
Thus, if we want an understanding of natural language as a whole we must be able to extrapolate meaning from sequence we have never seen before.

There are several approaches to avoiding these zero events.
One family of approaches is called \textit{smoothing}, where every possible sequence is provided some small probability mass.
An example of smoothing is called \textit{additive smoothing} \cite{chen_goodman_1999,goodman_2001} where zero probabilities are avoided by assuming that each event occurs at least $\alpha$ times in addition to its observed occurrences in the corpus.
The MLE estimate is modified as
\begin{equation}
    \hat p (w_{i+1} \mid w_{i-k:i}) = \frac{\countf{w_{i-k:i+1}} + \alpha}{\countf{w_{i-k:i}} + \alpha v} \label{eq:language-models:alpha-smoothing}
\end{equation}
where $v$ is the size of the token vocabulary and $0 < \alpha \leq 1$. Another family of approaches is using \textit{back-off}, where if a sequence $w_{i-k:i}$ is not observed, the model falls back to using $w_{i-k-1:i}$\cite{chen_goodman_1999,goodman_2001,JelMer80}.

\subsection{Feed-Forward Neural Network Models}\label{sec:language-models:ffnns}
Using feed-forward neural networks were first seriously applied to modeling natural language by Yoshua Bengio in 2003 \cite{bengio2003}.
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[
        node distance=2.6cm,
        >={Stealth[length=2mm]},
        layer/.style={draw, minimum width=1.5cm},
        word_vect/.style={draw, dashed, minimum height=2cm}
        ]

        \node[word_vect] (w1) {$C(w_1)$};
        \node[word_vect, below=1mm of w1, anchor=north] (w2) {$C(w_2)$};

        \node[below=3mm of w2, anchor=north] (ellipsis) {$\vdots$};

        \node[word_vect, below= 3mm of ellipsis, anchor=north] (wk) {$C(w_k)$};

        \node[left=1cm of w1] (w1_label) {$w_1$};
        \node[left=1cm of w2] (w2_label) {$w_2$};
        \node[left=1cm of wk] (wk_label) {$w_k$};

        \node[layer, fit={(w1) (w2) (ellipsis) (wk)}, label=below:{$\vec x \in \R^{km}$}] (input) {};

        \node[layer, label=below:{$\tanh$}, right of=input, minimum height=5cm] (hidden) {};
        \node[layer, label=below:{softmax}, right of=hidden, minimum height=10cm] (softmax) {};
        \node[right=1.2cm of softmax] (output) {$P(w_{k+1} \mid w_{1:k})$};

        \draw[->] (w1_label) -- (w1);
        \draw[->] (w2_label) -- (w2);
        \draw[->] (wk_label) -- (wk);

        \draw[->] (input) -- (hidden);
        \draw[->] (hidden) -- (softmax);
        \draw[->] (softmax) -- (output);
    \end{tikzpicture}
    \caption{Bengio's feed-forward language model architecture}\label{fig:language-models:ffnnlm}
\end{figure}
Bengio used a feed-forward network, where each word in the vocabulary is mapped to a $m$-dimensional vector in a continuous vector space.
\sarcasm{The paper~\cite{bengio2003} is light on the details of the mapping $C : V \to \R^m$.
    \cite{pappas_meyer_2012} fills in some of the details, but still doesn't explicitly explain how $C$ is constructed.
    It seems to indicate that $C$ is \textit{learned} at the same time as the rest of the network?
    So does that means that the actual inputs are the word indices?}
Then each word in the sequence $w_{i-k:i}$ is mapped to their corresponding vectors, which are then concatenated to form the $k \cdot m$-dimensional input vector for the neural network.

The softmax output layer shown in \autoref{fig:language-models:ffnnlm} is the most computational aspect of Bengio's language model ---
given a vocabulary of size $v$, one softmax computation requires a matrix-vector multiplication with a matrix sized $d_\text{hidden} \times v$, followed by $v$ exponentiations.
This makes using large vocabularies prohibitively expensive~\cite{goldberg_2017}.

\subsection{Recurrent Neural Network Models}\label{sec:language-models:rnns}

For several years, Bengio's feed-forward language model shown in \autoref{fig:language-models:ffnnlm} was the state-of-the-art neural network language model.
Then in 2010, Tomas Mikolov~\cite{Mikolov2010RecurrentNN,Mikolov2011RecurrentNN,Mikolov2012ContextDR} proposed a recurrent neural network (RNN) architecture that made major advancements in model performance.

The use of a recurrent network over the standard feed-forward models has several advantages~\cite{goldberg_2017}.
Importantly, they allow representing arbitrary-length sequences as fixed-size vectors.
This is of particular importance in modeling natural language, where the lengths of sentences are quite varied.
RNNs also abandon the $k$-th order Markov assumption in \autoref{eq:language-models:k-order-sequence-probability} that Bengio's FFNN language model follows.
The use of recurrent networks allows the output probabilities of a neural network language model to be conditioned on the \textit{entire} preceding context, instead of a fixed $k$ number of preceding tokens.

Recurrent architectures are able to do by by inputting \textit{a single word at a time} to the network, rather than a collection of $k$-words as in \autoref{fig:language-models:ffnnlm}.
Doing so allows recurrent networks to consider the past context of any words previously input to the network by storing internal context vectors, and then using the last word's context as an input to the network, in addition to the next word.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[
        node distance=3cm,
        >={Stealth[length=2mm]},
        layer/.style={draw, minimum width=1.8cm},
        ]
        \node[draw, dashed, minimum width=1.8cm, minimum height=6cm] (input) {$x(t)$};
        \node[draw, dashed, minimum width=1.8cm, minimum height=3cm, below=3mm of input] (prev_context) {$C(t-1)$};

        \node[layer, fit={(input) (prev_context)}] (input_layer) {};
        \node[layer, minimum height=3cm, right of=input_layer, label=below:{sigmoid}] (context) {$C(t)$};
        \node[layer, minimum height=8cm, right of=context, label=below:{softmax}] (softmax) {$y(t)$};

        \node[left=1.2cm of input] (input_label) {$w(t)$};
        \node[right=1.2cm of softmax] (output_label) {$P(w(t+1) \mid w(t), C(t-1))$};

        \draw[->] (input_label) -- (input);
        \draw[->] (input_layer) -- (context);
        \draw[->] (context) -- (softmax);
        \draw[->] (context.east) to [out=350, in=0, looseness=1.6] (prev_context);
        \draw[->] (softmax) -- (output_label);
    \end{tikzpicture}
    \caption{Mikolov's recurrent language model architecture}\label{fig:language-models:rnnlm}
\end{figure}

Mikolov's architecture is shown in \autoref{fig:language-models:rnnlm}.
The word $W(t)$ is converted to a one-hot encoding\footnote{
    One-hot encoding is a strategy that uses a boolean vector with an index for each word in the vocabulary.
    To represent the word $w$ from vocabulary $V$ with index $i$ in a one-hot encoded vector $x$, we set $x_i = 1$, and every other index is set to zero.}
$x(t)$, which is then concatenated with the context $C(t-1)$ from the previous time step\footnote{
    As with all recurrence relations, care should be taken with the initial conditions.
    Mikolov set $C(0)$ to a vector of small real values close to 0.1.},
and used as the input to a hidden context layer.
The context layer is then fed to the next iteration's input and the current iteration's softmax output layer to output a probability distribution for the next word $W(t+1)$.
Mikolov notes that the size of the hidden context layer should be proportional to the size of the data set.
This results in training times that increase more than linearly with the size of the training set.
Another problem with RNN language models is that the gradients in RNN models tend to vanish and explode during training \cite{jing_survey}.

\subsection{Long Short Term Memory Models}\label{sec:language-models:lstms}

In light of the vanishing gradients of RNN models, the use of a Long Short Term Memory (LSTM) network architecture has been found to be effective \cite{lstm}.
LSTM models are a class of recurrent networks.
\autoref{fig:language-models:rnn-layer} shows the basic structure of one layer in a recurrent neural network.
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[
        node distance=3cm,
        >={Stealth[length=2mm]},
        layer/.style={draw, minimum width=1.8cm},
        ]
        \node[draw, dashed, minimum width=1.8cm, minimum height=4cm] (x) {$x(t)$};
        \node[draw, dashed, minimum width=1.8cm, minimum height=2cm, below=3mm of x] (context) {$y(t-1)$};
        \node[draw, fit={(x) (context)}] (input) {};

        \node[left of=x] (x_label) {$x(t)$};

        \node[draw, label=below:{activation}, right of=input] (activation) {$f$};
        \node[right=5mm of activation] (phantom) {};
        \node[draw, label=below:{RNN layer}, fit={(input) (activation) (phantom)}, inner sep=5mm] (layer) {};

        \node[right=1cm of layer] (output) {$y(t)$};

        \draw[->] (x_label) -- (x);
        \draw[->] (input) -- node[midway, above] {$W$} (activation);
        \draw[->] (activation) -- (output);
        \draw[->] (activation.east) to [in=0, out=350, looseness=2.2] (context.east);

    \end{tikzpicture}
    \caption{The basic RNN layer}\label{fig:language-models:rnn-layer}
\end{figure}
Notice that the architecture is now temporal --- each input and output is labeled with a timestep --- and that the key difference from the basic layers of \autoref{fig:neural-networks:multilayer-perceptron} is that the recurrent layer also considers $y(t-1)$ in addition to the input values $x(t)$ when producing $y(t)$.

As \autoref{fig:language-models:lstm-layer} shows, LSTM layers have many of the same characteristics --- a single vector $x(t)$ input, a single vector $y(t)$ output, and the general recurrent property that internal context wraps back around as input for the next time step.

In an RNN model, the context that wraps around is the output vector $y(t)$, but in LSTM models, both the output $y(t)$ \textit{and} a new, hidden context vector $c(t)$ wraps around.
LSTM models also differ in the addition of several gates to control how much context to remember, when it should forget the context, and when it should output the value of the activation function.
As this is a slightly more involved architecture, some amount of explanatory remarks are necessary.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[>={Stealth[length=2mm]}]
        \node[draw] (forget) {forget gate};
        \node[draw, right=5mm of forget] (input) {input gate};
        \node[draw, right=5mm of input] (features) {new features};
        \node[draw, right=5mm of features] (output) {output gate};

        \node[draw, circle, above=2.5cm of forget] (forget_mult) {$\times$};
        \node[draw, circle, above=1cm of input]  (input_mult)  {$\times$};
        \node[draw, circle, above=1cm of output] (output_mult) {$\times$};
        \node[draw, ellipse, left=0.8cm of output_mult] (state_tanh) {$\tanh$};
        \node[draw, circle] (input_add) at (forget_mult -| input_mult)  {$+$};

        \node[below of=forget, inner sep=0, outer sep=0, xshift=4mm] (y_input_phantom) {};
        \node[below=1.5cm of forget, inner sep=0, outer sep=0] (x_input_phantom) {};
        \node[circle, fill, inner sep=1.5pt] (state_tanh_phantom) at (input_add -| state_tanh) {};

        \node[left=2.2cm of forget_mult] (c_prev) {$c(t-1)$};
        \node (y_prev) at (y_input_phantom -| c_prev) {$y(t-1)$};
        \node[right=8.4cm of input_add] (c_next) {$c(t)$};

        \node (y_next) at (y_prev -| c_next) {$y(t)$};
        \node[above=3.5cm of output_mult, label=above:{\scriptsize to next layer}] (y) {$y(t)$};
        \node[below=1.5cm of x_input_phantom, label=below:{\scriptsize from previous layer}] (x) {$x(t)$};

        \draw[->, dashed] (c_prev) -- (forget_mult);
        \draw[->] (forget_mult) -- (input_add);
        \draw[->] (input_add) -- (c_next);

        \draw[->] (forget) -- (forget_mult);
        \draw[->] (input) -- (input_mult);
        \draw[->] (input_mult) -- (input_add);
        \draw[->] (state_tanh_phantom) -- (state_tanh);
        \draw[->] (state_tanh) -- (output_mult);
        \draw[->] (output) -- (output_mult);

        \node[left=1cm of y_next, inner sep=0, outer sep=0] (output_bend_phantom) {};

        \draw[->] (output_mult) -| (output_bend_phantom) -- (y_next);

        \draw[->] (output_mult) -- (y);
        \draw[->] (features) to [in=0, out=90] (input_mult);

        \draw[->] (x) -- (x_input_phantom) node[circle, fill, inner sep=1.5pt] {} -| (output);
        \draw[->] (x_input_phantom -| features) node[circle, fill, inner sep=1.5pt] {} -| (features);
        \draw[->] (x_input_phantom -| input) node[circle, fill, inner sep=1.5pt] {} -| (input);
        \draw[->] (x_input_phantom) node[circle, fill, inner sep=1.5pt] {} -- (forget);

        \draw[->, dashed] (y_prev) -- (y_input_phantom) node[circle, fill, inner sep=1.5pt] {} -| (output.320);
        \draw[->, dashed] (y_input_phantom -| input.320) node[circle, fill, inner sep=1.5pt] {} -| (input.320);
        \draw[->, dashed] (y_input_phantom -| features.320) node[circle, fill, inner sep=1.5pt] {} -| (features.320);
        \draw[->, dashed] (y_input_phantom -| forget.320) node[circle, fill, inner sep=1.5pt] {} -| (forget.320);

        \node[draw, fit={(forget) (input) (features) (output) (forget_mult)
                (input_mult) (output_mult) (y_input_phantom) (x_input_phantom)},
        inner sep=1cm, label=below:{LSTM layer}] (layer) {};
    \end{tikzpicture}
    \caption{An LSTM layer}\label{fig:language-models:lstm-layer}
\end{figure}

LSTM networks have an internal state called $c(t)$, and an output $y(t)$.
The internal state is recurrently passed from the output of the layer to its input on the next time step, as is the output $y(t)$.
However, the output is also passed to the next layer in the network.

The internal state can be modified by the network, in a manner that is controlled by several different \textit{gates}.
Each gate, is a normal feed-forward network layer with a sigmoid activation function.
The layer to learn new features is a normal feed-forward layer too, but its activation function is most often the hyperbolic tangent.

Since the sigmoid function outputs values between 0 and 1, and each of the gates are immediately followed by an element-wise multiplication ($\otimes$), the gates are aptly named --- as they control how much of a signal passes.
The forget gate controls how much of the previous timestep's internal state is remembered, while the input gate controls how much of the new features are added element-wise ($\oplus$) to the internal state.
Finally, the internal state is passed element-wise through a hyperbolic tangent activation function before being multiplied by the results of the output gate.

We can formalize this architecture mathematically as
\begin{align}
    f_g(t) & = \sigma\left(W_f \cdot [y(t-1), x(t)] + b_f\right)                          & \text{forget gate}  \nonumber \\
    i_g(t) & = \sigma\left(W_i \cdot [y(t-1), x(t)] + b_i\right)                          & \text{input gate}   \nonumber \\
    o_g(t) & = \sigma\left(W_o \cdot [y(t-1), x(t)] + b_o\right)                          & \text{output gate}  \nonumber \\
    g(t)   & = \tanh \left(U   \cdot [y(t-1), x(t)] + d  \right)                          & \text{new features} \nonumber \\
    c(t)   & = \left(f_g(t) \otimes c(t-1)\right) \oplus \left(i_g(t) \otimes g(t)\right) & \text{internal state}         \\
    y(t)   & = o_g(t) \otimes \tanh\left(c(t)\right)                                      & \text{output}
\end{align}
where $[y(t-1), x(t)]$ is the concatenation of the $y(t-1)$ and $x(t)$ vectors, $\otimes$ and $\oplus$ are element-wise multiplication and addition respectively, $W_f, W_i, W_o$, and $U$ are weight matrices, and $b_f, b_i, b_o$, and $d$ are bias vectors \cite{jing_survey}.

The application of LSTM networks to modeling natural language is a relatively recent development \cite{Sundermeyer2012LSTMNN}.
\autoref{fig:language-models:lstm-lm} shows the proposed neural network topology.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[
        node distance=2.6cm,
        >={Stealth[length=2mm]},
        layer/.style={draw, minimum width=1.5cm},
        ]

        \node[layer, minimum height=10cm, label=below:{input}] (input) {$\vec x$};
        \node[layer, label=below:{dense}, right of=input, minimum height=5cm] (dense) {};
        \node[layer, label=below:{LSTM}, right of=dense, minimum height=6cm] (lstm) {};
        \node[layer, label=below:{softmax}, right of=lstm, minimum height=10cm] (softmax) {};
        \node[right=1.2cm of softmax] (output) {$P(w_{k+1} \mid w_{1:k})$};

        \draw[->] (input) -- (dense);
        \draw[->] (dense) -- (lstm);
        \draw[->] (lstm) -- (softmax);
        \draw[->] (softmax) -- (output);

        \draw[->] (lstm) to [in=95, out=85, looseness=6] node[midway, above] {\scriptsize context} (lstm);
    \end{tikzpicture}
    \caption{The LSTM language model architecture proposed by \cite{Sundermeyer2012LSTMNN}}\label{fig:language-models:lstm-lm}
\end{figure}

The input vector $\vec x$ is one-hot encoded, which means that, given a vocabulary of size $n$, $\vec x$ is $n$-dimensional and zero everywhere except for the index corresponding to the word $\vec x$ represents where its value is one.
This is a common word representation for neural network models, and in the implementations of the network training and computation the supporting linear algebra and optimization libraries are free to abuse the sparse nature of the input vector.

The first hidden layer, in effect, projects the one-hot encoded input vector to a continuous vector space.
Then this continuous word representation is passed through the LSTM network that is able to keep track of the context between different $\vec x$ vectors (each representing a word in the vocabulary) fed to the network.
Finally, note that the softmax output layer outputs a probability value for each word in the vocabulary, making it the dominating computational factor in the network training \cite{Sundermeyer2012LSTMNN}.

%% TODO: Wrap in a "Data Representation" section?
\subsection{Word Representations}\label{sec:language-models:word-representations}
%% TODO: Remove this blurb.
Sections \autoref{sec:language-models:markov} through \autoref{sec:language-models:lstms} form an introduction to neural network language modeling.
Until recently, the LSTM approaches from \autoref{sec:language-models:lstms} were the state-of-the-art, however large advances have been made recently, in the use of continuous word representations (\autoref{sec:language-models:word-representations}), attention (\autoref{sec:language-models:attention}), transformers (\autoref{sec:language-models:transformers}), BERT (\autoref{sec:language-models:bert}), and GPT-2 (\autoref{sec:language-models:gpt-2}).

\TODO{See chapter 14 of \cite{eisenstein_2019}.}

\subsection{word2vec}\label{sec:language-models:word2vec}
\subsection{Global Vectors for Word Representation}\label{sec:language-models:glove}
\subsection{doc2vec}\label{sec:language-models:doc2vec}

\subsection{Encoders and Decoders}\label{sec:language-models:encoders-decoders}
\TODO{See \cite{eisenstein_2019}.}
\subsection{seq2seq}\label{sec:language-models:seq2seq}
\TODO{See \cite{eisenstein_2019}.}

\subsection{Attention}\label{sec:language-models:attention}

\TODO{See section 18.3.1 of \cite{eisenstein_2019}}

\subsection{Transformers}\label{sec:language-models:transformers}
\TODO{See \cite{eisenstein_2019}.}
%% TODO: Move up to word embeddings?
\subsection{Byte Pair Encodings}\label{sec:language-models:bpe}

%% Specific Language Models
\subsection{Bidirectional Encoder Representations from Transformers}\label{sec:language-models:bert}
\subsection{Generative Pre-Training (I)}\label{sec:language-models:gpt}
\subsection{Generative Pre-Training (II)}\label{sec:language-models:gpt-2}
