% !TeX root = ../introduction.tex
\subsection{Text Generation}\label{sec:introduction:text-generation}

The discussion in \autoref{sec:introduction:language-models} has been limited to the use of various methods to produce statistical models for understanding natural language.
Here, we discuss the generative application of language models\footnote{
    Other applications of language models include
    \begin{itemize}
        \item Machine translation, where a language model is used to translate between two natural languages
        \item Question answering systems
        \item Summary generation
        \item Named Entity Recognition (NER), where dates, times, people, corporations, places, etc., are extracted and tagged with their respective categorization
        \item Sentament analysis
        \item Image captioning
        \item Lemmatization, where words such as `best' and `better' are reduced to their lemma `good'
        \item Part of speech tagging, where words such as `address' are examined with their surrounding context to determine their part of speech
    \end{itemize}
    and many others.
}.

\subsubsection{The Generative Use of Language Models}\label{sec:text-generation:language-models}

Recall from \autoref{sec:introduction:language-models} that a language model $\model$ is a statistical model of a sequence of tokens $\{w_1, w_2, w_3, \dots\}$ where the probability of the token $w_{k+1}$ is conditioned on \textit{all} of the preceding tokens $w_{1:k}$, denoted $\model\left(w_{k+1} \mid w_{1:k}\right)$.
Since the output of a language model is a probability distribution, we sample the distribution generated by $\model(w_2 \mid w_1)$ to generate $w_2$ conditioned on $w_1$.
Then we sample the distribution generated by $\model(w_3 \mid w_{1:2})$ to generate $w_3$ conditioned on $\{w_1, w_2\}$, and so on \cite{goldberg_2017}.
The method used to sample the probability distribution can vary, as does the quality of the generated sequence.

The sequence of tokens $\{w_1, w_2, w_3, \dots\}$ can be a sequence of characters, or a sequence of words.
Often, we also insert meta-tokens into the sequence, such as punctuation, a start-of-sequence tag (\verb|<s>|), and an end-of-sequence tag (\verb|</s>|) so that the language model can condition its predictions based on even more information.

When sampling the probability distribution generated by $\model\left(w_{k+1} \mid w_{1:k}\right)$, one strategy is to always pick the token with the highest probability.
Another strategy might be to randomly sample the probability distribution.
This way the output of the generative network is more diverse and creative.

We do this by transforming the softmax distribution
\begin{equation}
    \softmax(\vec x) = \frac{\exp\left(\vec x\right)}{\sum\exp\left(\vec x\right)} \tag{\ref{eq:neural-networks:softmax}}
\end{equation}
with a given temperature \cite{hinton2015distilling}.
\begin{equation}
    \softmax(\vec x, T) = \frac{\exp\left(\vec x \middle/ T \right)}{\sum\exp\left(\vec x \middle/ T\right)} \label{eq:text-generation:softmax-temperature}
\end{equation}
This temperature transformation can occur during the training of the network, where the normal softmax activation layer is replaced with \autoref{eq:text-generation:softmax-temperature}.
We can also apply the temperature transformation \textit{after} training during the generation phase by piping the softmax output values through the temperature transformation
\begin{equation}
    \operatorname{sample}(\vec y, T) = \frac{\exp\left(\log(\vec y) \middle/ T\right)}{\sum\exp\left(\log(\vec y) \middle/ T\right)}\label{eq:text-generation:temperature-sampling}
\end{equation}
after \autoref{eq:neural-networks:softmax} has been applied to the logits $\vec x$.
Note that \[\operatorname{sample}(\softmax(\vec x), T)\] and \[\softmax(\vec x, T)\] are equivalent.

Lower temperature values produce a more extreme distribution profile, while higher temperatures smooth out the probability distribution.
This means that text generated with a lower temperature is more confident in its choices, but that it is also more conservative than text generated with a high temperature.
Likewise, text generated with a high temperature is more diverse because any peaks in the probability distribution get smoothed out.
However, the added creativity and diversity that using a higher temperature provides comes at the risk of \FIXME{generating nonsense.}

\TODO{Discuss beam search}

\subsubsection{textgenrnn}\label{sec:text-generation:textgenrnn}
\subsubsection{Generative Pre-Training (II)}\label{sec:text-generation:gpt-2}
\subsubsection{Fine-Tuning Generative Pre-Training (II) Models}\label{sec:text-generation-fine-tuning-gpt-2}

%% TODO: The "something new" aspect of this thesis I think will be fine tuning GPT-2 on haiku.
